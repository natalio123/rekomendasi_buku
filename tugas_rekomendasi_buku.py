# -*- coding: utf-8 -*-
"""Tugas_Rekomendasi_buku

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kzVN3ki5uxxI9OWuH0GF7mE5YdIZUrFm

# Laporan Proyek Sistem Rekomendasi Buku - Natalio Tumuahi

Import Library
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
from sklearn.neighbors import NearestNeighbors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import csr_matrix

"""## **Data Loading**"""

user = pd.read_csv('https://raw.githubusercontent.com/natalio123/rekomendasi_buku/master/dataset/Users.csv', delimiter=';')
book = pd.read_csv('https://raw.githubusercontent.com/natalio123/rekomendasi_buku/master/dataset/Books.csv', delimiter=';')
rating = pd.read_csv('https://raw.githubusercontent.com/natalio123/rekomendasi_buku/master/dataset/Ratings.csv', delimiter=';')

print('Jumlah data users: ', len(user))
print('Jumlah data books: ', len(book))
print('Jumlah data ratings: ', len(rating))

"""## **Data Understanding**

"""

user.info()

# cek duplikate dari file users.csv
duplikat1= user.duplicated().sum()
duplikat1

# cek missing value
missing_value_user = user.isnull().mean() * 100
missing_value_user

# Konversi dan drop sementara NaN hanya untuk keperluan visualisasi outlier
user['Age'] = pd.to_numeric(user['Age'], errors='coerce')
sns.boxplot(x=user['Age'].dropna())

"""Insight: <br>
Berdasarkan analisis boxplot pada fitur Age, didapatkan bahwa data usia pengguna tersebar antara sekitar 5 hingga 60 tahun, dengan median usia berada di sekitar 35 tahun. Namun, terdapat banyak outlier signifikan di atas 80 tahun, bahkan hingga mendekati 250 tahun, yang menunjukkan adanya nilai-nilai anomali atau kesalahan input data. Hal ini menandakan perlunya pembersihan data untuk memastikan analisis dan model yang lebih akurat.
"""

user.describe(include='all')

book.info()

# cek duplikat dari dataset books
book.duplicated().sum()

# cek missing value
book.isnull().mean() * 100

# cek outliers
sns.boxplot(x=book['Year'])

"""Insight: <br>
Berdasarkan analisis boxplot pada fitur Year, dapat dilihat bahwa sebagian besar data tahun terdistribusi secara normal di sekitar tahun 1900 hingga 2020. Namun, terdapat beberapa outlier ekstrem yang mencurigakan, seperti tahun 0, 1000, 1400, dan 1700, yang secara realistis tidak mungkin merepresentasikan tahun terbit buku modern. Kehadiran nilai-nilai tersebut menunjukkan adanya kesalahan entri data atau nilai yang hilang yang digantikan dengan default, sehingga perlu dilakukan pembersihan atau imputasi data agar analisis menjadi valid.

"""

book.describe(include='all')

rating.info()

# cek duplikat
rating.duplicated().sum()

# cek missing value
rating.isnull().mean() * 100

rating.describe(include='all')

# ubah tipe data User-ID menjadi string
rating['User-ID'] = rating['User-ID'].astype(str)
rating.info()

# cek outliers
sns.boxplot(x=rating['Rating'])

"""Insight: <br>
Berdasarkan analisis boxplot pada fitur Rating, dapat disimpulkan bahwa data rating tersebar merata dari nilai 0 hingga 10, tanpa adanya outlier yang mencolok. Median berada di sekitar nilai 5, menunjukkan bahwa sebagian besar pengguna memberikan rating pada tingkat yang moderat atau netral. Penyebaran yang luas ini mengindikasikan adanya variabilitas penilaian yang tinggi, mencerminkan perbedaan preferensi dan pendapat pengguna terhadap buku yang dinilai.

## **Data Preprocessing**
"""

# Hapus outlier variabel user
df_numeric = user['Age']
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
df = user[~((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR)))]

# cek ukuran dataset
user.shape

# Hapus duplikat variabel book
book = book.drop_duplicates()
book.shape

# Isi missing value dengan median
user['Age'].fillna(user['Age'].median(), inplace=True)

# Ubah tipe data menjadi integer
user['Age'] = user['Age'].astype(int)

# Filter data dengan usia yang masuk akal (antara 5 sampai 80 tahun)
user = user[(user['Age'] >= 5) & (user['Age'] <= 80)]

# Hapus outlier variabel book
df_numeric = book['Year']
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
df = book[~((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR)))]

# cek ukuran dataset
book.shape

# Hapus missing value pada variabel book
book.dropna(inplace=True)
book.shape

# hapus rating 0
rating= rating[rating['Rating'] > 0]
rating.shape

# Fungsi untuk memisahkan inisial penulis yang digabung menjadi satu kata
def process_author_name(author_name):
  author_name = re.sub(r'(\w)\.(\s?)(\w)', r'\1 \3', author_name)
  author_name = re.sub(r'(\w)\.(\s?)(\w)', r'\1 \3', author_name)  # Memastikan titik dihapus antara setiap inisial

  # Menghapus titik di akhir nama setelah inisial (contoh: "Mark P. O.")
  author_name = re.sub(r'(\w)\.(\s?)(\w)', r'\1 \2', author_name)  # Mengubah "P." menjadi "P"

  # Menangani apostrof, jika ada
  author_name = re.sub(r"(\w)'(\w)", r"\1 \2", author_name)  # Mengubah "D'Este" menjadi "D Este"

  return author_name

book['Author'] = book['Author'].apply(process_author_name)
book.head()

# Gabungkan data buku dan rating
book_ratings = rating.merge(book, on='ISBN')
book_ratings.head()

# Gabungkan fitur judul dan penulis dari variabel book
book['combined_features'] = book['Title'] + ' ' + book['Author']
book

"""## **Model Development**

"""

# Bangun model Content Based Filtering dengan TF-IDF dan model_knn
# Gunakan hanya 9999 data pertama dikarenakan RAM tidak mencukupi
sampled_books = book.iloc[:9999].copy()

# TF-IDF vectorization
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(sampled_books['combined_features'])

# Hitung cosine similarity antar semua buku dalam subset ini
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

def get_recommendations(title, n_recommendations=5):
    if title not in sampled_books['Title'].values:
        return "Judul buku tidak ditemukan dalam dataset."

    # Mendapatkan indeks buku yang sesuai dengan title
    idx = sampled_books[book['Title'] == title].index[0]

    # Mendapatkan pairwise similarity skor untuk semua buku dengan buku yang diberikan
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Mengurutkan skor kesamaan berdasarkan nilai tertinggi
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Mendapatkan indeks 10 buku yang paling mirip
    sim_scores = sim_scores[1:11]

    # Mendapatkan indeks buku yang direkomendasikan
    book_indices = [i[0] for i in sim_scores]

    # Mengembalikan judul buku yang direkomendasikan
    return book['Title'].iloc[book_indices]

from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors

# Encode user dan buku jadi indeks integer
from sklearn.preprocessing import LabelEncoder

user_enc = LabelEncoder()
book_enc = LabelEncoder()

book_ratings['user_enc'] = user_enc.fit_transform(book_ratings['User-ID'])
book_ratings['book_enc'] = book_enc.fit_transform(book_ratings['Title'])

# Buat sparse matrix (book × user)
sparse_matrix = csr_matrix((
    book_ratings['Rating'],
    (book_ratings['book_enc'], book_ratings['user_enc'])
))

# Bangun model KNN berbasis cosine
model = NearestNeighbors(metric='cosine', algorithm='brute')
model.fit(sparse_matrix)

def get_book_recommendations(title, n_recommendations=5):
    # Pastikan judul ada dalam label encoder
    if title not in book_enc.classes_:
        return "Judul buku tidak ditemukan."

    # Dapatkan indeks buku dari judul
    book_idx = book_enc.transform([title])[0]

    # Cari tetangga terdekat dari buku tersebut
    distances, indices = model.kneighbors(sparse_matrix[book_idx], n_neighbors=n_recommendations + 1)

    # Lewati indeks pertama (buku itu sendiri)
    recommended_indices = indices[0][1:]

    # Ubah indeks kembali ke judul
    recommended_titles = book_enc.inverse_transform(recommended_indices)

    return recommended_titles

"""## **Evaluation**

"""

# Bagi data ke training dan test set
train_data, test_data = train_test_split(book_ratings, test_size=0.2, random_state=42)

# Buat sparse matrix untuk training set
train_sparse = csr_matrix((
    train_data['Rating'],
    (train_data['book_enc'], train_data['user_enc'])
))

# Fit model ke training data
model.fit(train_sparse)

# Evaluasi prediksi rating
predictions = []
true_ratings = []

for _, row in test_data.iterrows():
    book_idx = row['book_enc']
    user_idx = row['user_enc']

    # Skip jika tidak ada di training matrix
    if book_idx >= train_sparse.shape[0] or user_idx >= train_sparse.shape[1]:
        continue

    # Ambil tetangga terdekat (termasuk dirinya)
    distances, indices = model.kneighbors(train_sparse[book_idx], n_neighbors=6)

    # Ambil rating dari tetangga selain dirinya
    neighbor_ratings = []
    for neighbor in indices[0][1:]:
        rating = train_sparse[neighbor, user_idx]
        if rating != 0:
            neighbor_ratings.append(rating)

    if neighbor_ratings:
        pred_rating = np.mean(neighbor_ratings)
        predictions.append(pred_rating)
        true_ratings.append(row['Rating'])

# Hitung MAE
mae = mean_absolute_error(true_ratings, predictions)
print(f"Mean Absolute Error (MAE): {mae:.4f}")

"""Insight: <br>
Nilai Mean Absolute Error (MAE) sebesar 0.8171 menunjukkan bahwa rata-rata selisih antara rating yang diprediksi oleh model dan rating sebenarnya adalah sekitar 0.82 poin pada skala 0–10. Ini berarti model collaborative filtering yang digunakan mampu memberikan prediksi yang cukup dekat dengan nilai sebenarnya.
"""

# Visualisasi: Error Distribution
errors = np.abs(np.array(true_ratings) - np.array(predictions))
plt.figure(figsize=(8, 5))
sns.histplot(errors, bins=20, kde=True, color='salmon')
plt.title('Distribution of Absolute Errors')
plt.xlabel('Absolute Error')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""Insight: <br>
Berdasarkan visualisasi distribusi Absolute Errors, mayoritas kesalahan prediksi berada di sekitar nilai 0 hingga 2, dengan frekuensi tertinggi pada nilai mendekati 0. Ini menunjukkan bahwa model collaborative filtering umumnya memberikan prediksi rating yang cukup akurat terhadap nilai aslinya. Namun, masih terdapat sebagian kecil error yang lebih besar (hingga lebih dari 20), yang menjadi indikasi adanya outlier atau kasus prediksi yang kurang tepat
"""

# Judul buku yang ingin direkomendasikan
title = 'Clara Callan'
n_recommendations = 10

print(f"Showing recommendations for book: {title}")
print("=" * 40)

# Content-Based Filtering
print("Books with similar content (TF-IDF)")
print("-" * 40)
if title not in sampled_books['Title'].values:
    print("❌ Book title not found in TF-IDF dataset.")
else:
    content_recs = get_recommendations(title, n_recommendations)
    for book_title in content_recs:
        print(book_title)
print("-" * 40)

# Collaborative Filtering (KNN)
print("Top book recommendations from similar users (KNN)")
print("-" * 40)
if title not in book_enc.classes_:
    print("❌ Book title not found in KNN dataset.")
else:
    knn_recs = get_book_recommendations(title, n_recommendations)
    for book_title in knn_recs:
        print(book_title)
print("=" * 40)